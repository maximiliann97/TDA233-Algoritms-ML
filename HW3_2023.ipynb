{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7oRBp3B8d_6"
   },
   "source": [
    "# Home Assignment 3 in Deep Learning and Clustering [20 points]\n",
    "**Goal:** Try out image classification using fully-connected and convolutional neural networks, and cluster representations of state-of-the-art image classifiers. <br />\n",
    "**Grader:** Lovisa Hagstr√∂m <br />\n",
    "**Due Date:** 8/3 <br />\n",
    "**Submitted by:** üìù Maximilian Sal√©n, Axel Qvarnstr√∂m - 970105, 980728 - maximilian@live.se, axelqv@student.chalmers.se\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO2HIsJJ8eAC"
   },
   "source": [
    "# Read this before starting\n",
    "\n",
    "## General guidelines \n",
    "* Answer all fields marked with üìù. This includes\n",
    "    * your name, personal number and email address above, and\n",
    "    * all later fields marked with \"üìù Your answer here:\".\n",
    "* Feel free to add more cells if needed.\n",
    "* All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook does not have to be run. The code in the notebook should reproduce the plots/results if we choose to run it. \n",
    "    * Do NOT hand in an assignment that isn't runnable!\n",
    "* Do not modify the provided code unless told to do so.\n",
    "* All tables and other additional information should be included in this notebook.\n",
    "* Before submitting, make sure that your code can run on another computer, i.e. that all plots can show on another computer including all your writing. It is also good to check if your code can run on Google Colab.\n",
    "* **Submit your solutions as notebook file (`.ipynb`) and in HTML format (`.html`).** To export this notebook to HTML format click `File` $\\rightarrow$ `Download as` $\\rightarrow$ `HTML`.\n",
    "\n",
    "\n",
    "> **Note:** Training neural networks is computationally demanding and may take time if you run it on your laptop. Running the code in Google Colab will likely be faster and you can even get access to a GPU.\n",
    "\n",
    "> **Note:** To enable GPU hardware accelartion in Google Colab, click the `Change runtime type` field in the `runtime` drop-down menu, then choose `GPU` under hardware acceleration.\n",
    "\n",
    "> **Note:** If you are using Google Colab and you would like to export the notebook to HTML format, you need to first download it through `File` $\\rightarrow$ `Download as` $\\rightarrow$ `ipynb` and then use the standard tool for Jupyter Notebook conversion, nbconvert: jupyter nbconvert --to html notebook.ipynb (you need to run it from the command line). For more info (and example), see [here](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yct6UOPN8eAD"
   },
   "source": [
    "## Required software\n",
    "\n",
    "For this assignment you will need to install the following Python packages:\n",
    "\n",
    "- `pytorch`: Installation instructions can be found on the [pytorch homepage](https://pytorch.org/get-started/locally/) (make sure that you install it together with CUDA to enable GPU acceleration)\n",
    "- `torchvision`: Typically installed with pytorch\n",
    "- `numpy`: The fundamental package for scientific computing with Python (so fundamental there is a [Nature review](https://www.nature.com/articles/s41586-020-2649-2) on it) \n",
    "- `pandas`: Data analysis and manipulation tool\n",
    "- `matplotlib`: Visualization with Python\n",
    "- `pillow`: Image library to handle PIL images\n",
    "- `catsndogs`: The data sets we will be working with.\n",
    "\n",
    "> **Note:** In Google Colab you can install packages using   `!pip  <package_name>`\n",
    "\n",
    "> **Note:** In Google Colab several of these packages are preinstalled but it is a good habit to check if all required packages are installed beforehand and the installed versions of packages. Use `!pip list` to list packages installed by pip on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmouXMfVnQj5"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ3kgSkZ8eAE"
   },
   "source": [
    "## Deep Learning\n",
    "## Exercise 1: Backpropagation by hand [2 points]\n",
    "\n",
    "Consider the simple feed-forward neural network depicted in the figure below. This network\n",
    "consists of an input layer $\\mathbf{y}_0 = \\mathbf{x}$ with 3 features,  one hidden layer\n",
    "with activations $\\mathbf{y}_1$ and a two-dimensional output layer with activations $\\mathbf{y}_2 = \\hat{\\mathbf{y}}$.\n",
    "\n",
    "![Neural network illustration.](https://raw.githubusercontent.com/hampusgs/machine_learning/main/2022/simpleNN.png)\n",
    "\n",
    "The activations of a layer $k$ are computed by applying a linear transformation given by the weight matrix\n",
    "$\\mathbf{W}^{(k)}$ to the input activations $\\mathbf{y}_{k - 1}$ producing the intermediate values $\\mathbf{z}_k$:\n",
    "\n",
    "$$\n",
    "z_{k , j} = \\sum_i  y_{k - 1,i} w^{(k)}_{i, j}\\\\\n",
    "$$\n",
    "\n",
    "This is followed by the element-wise application of the layers'\n",
    "activation function $g_k$ to the intermediate values $\\mathbf{z}_k$:\n",
    "\n",
    "$$\n",
    "y_{k,j} = g_{k} (z_{k,j})\n",
    "$$\n",
    "\n",
    "> **Note:** Here we use the same notation as in the lecture slides, but several different notations exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zINsB2c-8eAE"
   },
   "source": [
    "### a)\n",
    "\n",
    "Given the derivatives of a loss term $E$ with respect to the activations of the output neurons \n",
    "$\\frac{dE}{dy_{2,j}}$ (and the variables/functions in the description above), derive expressions for the derivatives of the loss term with respect to the weights\n",
    "$w^{(k)}_{i,j}$ and activations $y_{k,j}$ of the remaining layers of the network. Simplify as much as possible. Please show all the calculation details.\n",
    "\n",
    "Also, to simplify the results, you are encouraged to reuse derivatives you have already computed in the expressions for the  downstream derivatives.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dE}{dw^{(2)}_{i, j}} = \\: ? \\\\\n",
    "\\frac{dE}{dy_{1,j}} = \\: ? \\\\\n",
    "\\frac{dE}{dw^{(1)}_{i, j}} = \\: ? \\\\\n",
    "\\frac{dE}{dy_{0,j}} = \\: ? \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If your calculations are correct, you should see that you can express the derivatives of the loss function \n",
    "around a given layer in the network using the derivatives from the next higher layer. This yields a simple\n",
    "recipe to successively compute the gradients in a feed forward neural network by starting at the last layer and\n",
    "then computing the gradients layer-by-layer as you move backwards through the network. This method is commonly\n",
    "referred to as **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_XHbYNP8eAF"
   },
   "source": [
    "#### üìù Your answer here: \n",
    "##### First derivative.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\frac{dE}{dw^{(2)}_{i, j}} = \\frac{dE}{dy_{2,j}}\\frac{dy_{2,j}}{dw_{i,j}^{(2)}} \\tag{1} \\\\\n",
    "&\\frac{y_{2,j}}{w_{i,j}^{(2)}} = g'(z_2)y_{1,j} \\tag{2}\n",
    "\\end{align}\n",
    "$$\n",
    "(2) in (1):\n",
    "$$\n",
    "\\frac{dE}{dw^{(2)}_{i, j}} = \\frac{dE}{dy_{2,j}}g'(z_2)y_{1,j} \\tag{3}\n",
    "$$\n",
    "\n",
    "##### Second derivative.\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\frac{dE}{dy_{1,j}} = \\frac{dE}{dy_{2,j}} \\frac{dy_{2,j}}{dy_{1,j}} \\tag{4} \\\\\n",
    "&\\frac{dy_{2,j}}{dy_{1,j}} = g'(z_2)w_{i,j}^{(2)} \\tag{5}\n",
    "\\end{align}\n",
    "$$\n",
    "(5) in (4):\n",
    "$$\n",
    "\\frac{dE}{dy_{1,j}} = \\frac{dE}{dy_{2,j}}g'(z_2)w_{i,j}^{(2)} \\tag{6}\n",
    "$$\n",
    "\n",
    "##### Third derivative.\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\frac{dE}{dw^{(1)}_{i, j}} = \\frac{dE}{dy_{2,j}}\\frac{dy_{2,j}}{dw_{i,j}^{(1)}} \\tag{7} \\\\\n",
    "&\\frac{dy_{2,j}}{dw_{i,j}^{(1)}} = \\frac{dy_{2,j}}{dy_{1,j}}\\frac{dy_{1,j}}{dw_{i,j}^{(1)}} \\tag{8} \\\\\n",
    "&\\frac{dy_{1,j}}{dw_{i,j}^{(1)}} = g'(z_1)y_{0,j} \\tag{9}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(5) and (9) in (8) which goes into (7):\n",
    "$$\n",
    "\\frac{dE}{dw^{(1)}_{i, j}} = \\frac{dE}{dy_{2,j}}g'(z_2)w_{i,j}^{(2)}g'(z_1)y_{0,j} \\tag{9}\n",
    "$$\n",
    "\n",
    "##### Last derivative.\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\frac{dE}{dy_{0,j}} = \\frac{dE}{dy_{2,j}}\\frac{dy_{2,j}}{dy_{0,j}} \\tag{10} \\\\\n",
    "&\\frac{dy_{2,j}}{dy_{0,j}} = \\frac{dy_{2,j}}{dy_{1,j}}\\frac{dy_{1,j}}{dy_{0,j}} \\tag{11} \\\\\n",
    "&\\frac{dy_{1,j}}{dy_{0,j}} = g'(z_1)w_{i,j}^{(1)} \\tag{12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(5) and (12) in (11):\n",
    "$$\n",
    "\\frac{dE}{dy_{0,j}} = \\frac{dE}{dy_{2,j}}g'(z_2)w_{i,j}^{(2)}g'(z_1)w_{i,j}^{(1)} \\tag{13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOb-QPhS8eAG"
   },
   "source": [
    "## Exercise 2: Counting parameters in networks [1 point]\n",
    "\n",
    "### a) [0.5 point]\n",
    "Imagine you apply a two layer fully connected network to a 32x32 rgb image. The hidden layer has dimension 512 and the output is of size 10. How many parameters are necessary? Include the bias parameters. Show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhM0XXpFnQj8"
   },
   "source": [
    "### b) [0.5 point]\n",
    "\n",
    "Apply the following network to the same image, how many parameters are needed? Include bias parameters and use no padding. Show your calculations.\n",
    "\n",
    "* Convolutional layer with 16 5x5 filters (stride 1).\n",
    "\n",
    "* Max pooling layer (2x2) (stride 2).\n",
    "\n",
    "* Convolutional layer with 32 5x5 filters (stride 1).\n",
    "\n",
    "* Fully connected layer to ouput of size 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2A6wmGz8eAH"
   },
   "source": [
    "#### üìù Your answer here:\n",
    "#### a)\n",
    "The number of parameters in a fully connected neural network depends on the number of neurons in each layer, including the bias term. The number of parameters in the weight matrix connecting layer i to layer j is given by the product of the number of neurons in layer i and the number of neurons in layer j, plus the number of bias terms in layer j.\n",
    "\n",
    "In this case, the input layer has 32 x 32 x 3 = 3072 neurons, the hidden layer has 512 neurons, and the output layer has 10 neurons. Therefore, the number of parameters in the weight matrix connecting the input layer to the hidden layer is:\n",
    "\n",
    "3072 x 512 + 512 = 1,572,864\n",
    "\n",
    "The number of parameters in the weight matrix connecting the hidden layer to the output layer is:\n",
    "\n",
    "512 x 10 + 10 = 5130\n",
    "\n",
    "Therefore, the total number of parameters in the network is:\n",
    "\n",
    "1,572,864 + 5130 = 1,578,994\n",
    "\n",
    "So there are 1,578,994 parameters in the network, including the bias terms.\n",
    "\n",
    "#### b)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&((5\\cdot5\\cdot3)+1) \\cdot 16 = 1216 \\\\\n",
    "&((5\\cdot5\\cdot2)+1) \\cdot 32 =  1632\\\\\n",
    "&10\\cdot800 + 10 = 8010 \\\\\n",
    "&\\text{Total amount of parameters} = 1216 + 1632 + 8010 = 10858\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qarm_MZr8eAI"
   },
   "source": [
    "## Exercise 3: Applying a filter to an image [0.5 point]\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Image:} \n",
    "\\begin{bmatrix}2 & 2 & -2 & 3 \\\\\n",
    "               -1 & 1 & -2 & 1 \\\\\n",
    "               1 & 3 & 1 & 1 \\\\\n",
    "               -1 & 2 & 1 & 1 \n",
    "\\end{bmatrix}\n",
    "\\ \\ \n",
    "\\text{Filter:}\n",
    "\\begin{bmatrix}-1 & 1\n",
    "\\\\-1 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Convolve the filter over the image and apply ReLU, use a stride of 2 with a bias of -2. What is the output? Give an explanation for the output, what is the filter detecting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfHmc12Z8eAJ"
   },
   "source": [
    "#### üìù Your answer here:\n",
    "Before sweeping the filter over the image\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}2 & 2 & -2 & 3 \\\\\n",
    "               -1 & 1 & -2 & 1 \\\\\n",
    "               1 & 3 & 1 & 1 \\\\\n",
    "               -1 & 2 & 1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "After sweeping with stride = 2:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}2 & 8 \\\\\n",
    "               5 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Appying the bias and Relu activation function:\n",
    "$$\n",
    "\\text{output} = \\begin{bmatrix}0 & 6 \\\\\n",
    "               3 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNY4Ix3U8eAJ"
   },
   "source": [
    "## Exercise 4: Recurrent neural networks [1.5 points]\n",
    "\n",
    "Consider a RNN, which maps a sequence of inputs $\\mathbf{x}_0, \\mathbf{x}_1, \\ldots$ to a sequence of outputs $\\mathbf{y}_0, \\mathbf{y}_1, \\ldots$. At each step $t$, the hidden state $\\mathbf{h}_t$ and output $\\hat{\\mathbf{y}}_t$ of the RNN are computed using\n",
    "\\begin{align}\n",
    "  \\mathbf{h}_t &= \\tanh(\\mathbf{W}_{\\mathbf{h}\\mathbf{h}} \\ \\mathbf{h}_{t -1} + \\mathbf{W}_{\\mathbf{x}\\mathbf{h}}\\ \\mathbf{x}_t ) \\\\\n",
    "  \\hat{\\mathbf{y}}_t &= \\mathbf{W}_{\\mathbf{h}\\mathbf{y}}\\ \\mathbf{h}_t\n",
    "\\end{align}\n",
    "  \n",
    "### a) [0.5 point]\n",
    "\n",
    "The RNN is applied to a sequence of two inputs $\\mathbf{x}_0, \\mathbf{x}_1$. Write down analytic expressions for the corresponding outputs $\\hat{\\mathbf{y}}_0, \\hat{\\mathbf{y}}_1$ assuming the initial hidden state to be the zero vector.\n",
    "\n",
    "### b) [0.5 point]\n",
    "\n",
    "Assume that the vectors $\\mathbf{x}_0, \\mathbf{x}_1$ have a length of 8, the hidden state $\\mathbf{h}_t$ a length of $16$, the output vectors $\\hat{\\mathbf{y}}_0, \\hat{\\mathbf{y}}_1$ a length of 1 and the bias is discarded. How many learnable parameters does the RNN described above have? How does this number depend on the length of the input sequence?\n",
    "\n",
    "### c) [0.5 point]\n",
    "\n",
    "Describe two difficulties that can occur when training RNNs. Elaborate descriptions will get higher scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOJKWNB68eAJ"
   },
   "source": [
    "#### üìù Your answer here:\n",
    "\n",
    "##### a)\n",
    "The initial hidden state $h_{-1}$ is a zero vector \n",
    "$$\n",
    "\\begin{align}\n",
    "&\\mathbf{h_0} = \\text{tanh}(\\mathbf{W_{hh}h_{-1}}+\\mathbf{W_{xh}x_0}) \\\\\n",
    "&\\mathbf{h_0} = \\text{tanh}(\\mathbf{W_{xh}x_0}) \\\\\n",
    "&\\hat{\\mathbf{y}}_0 = \\mathbf{W_{hy}}\\text{tanh}(\\mathbf{W_{xh}x_0})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now for the next output vector $\\hat{\\mathbf{y}}_1$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\mathbf{h_1} = \\text{tanh}(\\mathbf{W_{hh}h_{0}}+\\mathbf{W_{xh}x_1})\\\\\n",
    "&\\hat{\\mathbf{y}}_1 = \\mathbf{W_{hy}}\\text{tanh}(\\mathbf{W_{hh}h_{0}}+\\mathbf{W_{xh}x_1})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqMCJm0P8eAJ"
   },
   "source": [
    "## Practical exercises - Image Classification\n",
    "In this practical part of the assignment, you will develop a classification algorithm that predicts whether an image contains a cat or a dog. You wil do this using the `pytorch` deep learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxsSA-1y8eAK"
   },
   "source": [
    "### The data\n",
    "\n",
    "The data that you will be using in this exercise consists of images of cats and dogs. The dataset is available through the `catsndogs` Python package. The package automatically downloads the data and provides access to the image files in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "utskemCq8eAK",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catsndogs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24224\\2418004538.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatsndogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdogs\u001b[0m \u001b[1;31m# The lists of cat and dog images.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catsndogs'"
     ]
    }
   ],
   "source": [
    "from catsndogs.training import cats, dogs # The lists of cat and dog images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZiMpYi-8eAL"
   },
   "source": [
    "Below, a few examples of the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "blJCRTYOGhG9",
    "outputId": "5c633824-2aec-441b-cec2-e09d989271dd"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "f, axs = plt.subplots(2, 4, figsize = (8, 4))\n",
    "for i in range(4):\n",
    "    img = np.random.choice(cats)\n",
    "    ax = axs[0, i]\n",
    "    ax.set_title(\"A Cat\")\n",
    "    ax.imshow(Image.open(img))\n",
    "for i in range(4):\n",
    "    img = np.random.choice(dogs)\n",
    "    ax = axs[1, i]\n",
    "    ax.set_title(\"A Dog\")\n",
    "    ax.imshow(Image.open(img))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMWtypNS8eAM"
   },
   "source": [
    "## Getting started with pytorch\n",
    "\n",
    "The following part provides a brief introduction to the fundamentals of `pytorch`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBB1XoKC8eAM"
   },
   "source": [
    "### Why pytorch?\n",
    "\n",
    "As most other popular deep learning frameworks, `pytorch` provides the following features:\n",
    "\n",
    "- automatic differentiation,\n",
    "- GPU support,\n",
    "- flexible composition of neural network models,\n",
    "- numerous pre-defined network components and optimization methods.\n",
    "\n",
    "Pytorch strikes a good balance between flexibility, usability and performance, making it well suited for an introductory exercise as this one. There of course exist quite a few alternative frameworks, but the general concepts that you will learn in this exercise will apply also for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nJf-vt08eAM"
   },
   "source": [
    "### Accessing documentation\n",
    "\n",
    "Note that you can access source code documentation from inside the jupyter notebook using `?` and the `help` function. Documentation of the different torch modules can be found on the [pytorch home page](https://pytorch.org/docs/stable/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uN3u1kNz8eAM",
    "outputId": "d874ddd9-d65b-4cf2-ec11-fe9ee084fa88",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "help(torch.tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrBlO8IN8eAN"
   },
   "source": [
    "##### Tensors\n",
    "\n",
    "Tensors are a fundamental concept of `pytorch`, as well as most other deep learning frameworks. A tensor  designates a collection of elements that are organized on a multi-dimensional grid. You may think of them as a generalization of vectors or matrices: The elements in a vector are organized along 1 dimension, whereas in a matrix they are organized along 2 dimensions.\n",
    "\n",
    "A typical application of tensors is to hold images. As an example, we can load an image of a dog into a `torch.tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrcc_9WL8eAN"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
    "image_name = np.random.choice(dogs)\n",
    "dog = to_tensor(Image.open(image_name))\n",
    "print(\"The size of 'dog' is:\", dog.size())\n",
    "dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5_cP4zw8eAN"
   },
   "source": [
    "It becomes interesting when we start applying mathematical operations to tensors. For example we can compute the average of a cat and a dog. Note that all common mathematical operators (`+`, `-` `*`, `**`, ...) are defined on tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "2NzCebBv8eAN",
    "outputId": "ae444ff4-ae7c-4223-d8a3-102bbf0de938"
   },
   "outputs": [],
   "source": [
    "image_name = np.random.choice(cats)\n",
    "cat = to_tensor(Image.open(image_name))\n",
    "plt.imshow(to_pil_image(0.5 * (cat + dog)))\n",
    "plt.title(\"A cat/dog average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS14NpAy8eAO"
   },
   "source": [
    "### Common tensor operations\n",
    "\n",
    "There are many operations available on tensors and most of them follow the names used in `numpy`. In general, you can expect there to be an operation for most tasks at hand, so make sure you check the `pytorch` documentation search engine before you start cooking up something on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az4n-k3p8eAO"
   },
   "source": [
    "#### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrVtLO4F8eAO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "ones = torch.ones(10, 10)\n",
    "zeros = torch.zeros(10, 10)\n",
    "rand = torch.randn(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu4uMc-A8eAP"
   },
   "source": [
    "#### Mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPfqTB1k8eAP"
   },
   "outputs": [],
   "source": [
    "rand1 = torch.add(ones, rand)\n",
    "p = torch.sigmoid(rand)\n",
    "exp = torch.exp(rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUnif9gN8eAQ"
   },
   "source": [
    "#### Conversion from and to numpy arrays\n",
    "\n",
    "numpy arrays can be converted directly to pytorch tensors using the `torch.tensor` function.\n",
    "\n",
    "Converting `pytorch` tensors to numpy arrays can be done using the `numpy()` member function. If `pytorch` tracks the gradient of a tensor, then you will also need to call the `detach()` member function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BcbvPLn38eAQ",
    "outputId": "fdfb38af-8190-4bed-cc5d-0d3e57166441"
   },
   "outputs": [],
   "source": [
    "t_numpy = np.random.rand(2, 2)\n",
    "t = torch.tensor(t_numpy)\n",
    "print(\"Type of t:\", type(t_numpy))\n",
    "print(\"Type of t_pytorch:\", type(t))\n",
    "print(\"Type of t_pytorch.numpy():\", type(t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8bR8bNf8eAR",
    "outputId": "2e664b11-da80-4cb5-d144-4d92e7eff3a2"
   },
   "outputs": [],
   "source": [
    "t.requires_grad = True\n",
    "#t.numpy() # Doesn't work\n",
    "t.detach().numpy() # Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVyzMXDb8eAR"
   },
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "One of the core strengths of pytorch is that it let's you compute complex mathematical operations on tensors and compute their derivatives. Remember, that this is an important part of training neural networks: In order to minimize the loss function using gradient descent, it is of course required to first compute the gradients. Luckily, `pytorch`'s `autograd` module can take care of all the complicated calculations that are required to compute the gradients of neural networks.\n",
    "\n",
    "Computing gradients w.r.t to a given tensor involves the following steps:\n",
    "1. Create a tensor and set the `requires_grad` attribute to `True`,\n",
    "2. apply mathematical operations,\n",
    "3. call the `backward()` function of the result tensor to compute the gradients.\n",
    "\n",
    "> *Note:* Step 1 is not required for parameters of networks, whose gradients are computed by default when the model is in training mode.\n",
    "\n",
    "As an example, take the following operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bu1Ap2Tv8eAR"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-4, 4, 101, requires_grad=True)\n",
    "y = torch.sigmoid(x)\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "dzdx = x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "JXr9QIf58eAR",
    "outputId": "60c3e94d-8bd9-459d-a2af-ca1109f878f3"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "ax.plot(x.detach().numpy(),\n",
    "        y.detach().numpy(),\n",
    "        label = \"$\\sigma(x)$\")\n",
    "ax.plot(x.detach().numpy(),\n",
    "        dzdx.numpy(),\n",
    "        label = \"$?(x)$\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rYAxLOC8eAS"
   },
   "source": [
    "## Exercise 5: Derivatives of activation functions [0.5 points] \n",
    "\n",
    "Write down analytical expressions for the function $\\sigma(x)$ and $?$ shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxEOLhXk8eAS"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gApKtlG_8eAS"
   },
   "source": [
    "### GPU acceleration\n",
    "\n",
    "Training complex networks is a computationally demanding task. To shorten training times, calculations are typically performed on specialized hardware that was traditionally used to render 3D graphics on computers, so called graphic processing units (GPUs) or graphic cards. GPUs are in general more efficient in performing  highly-parallel computational tasks than CPUs, which are the chips that perform all 'standard' calculations in a PC. In `pytorch`, all oprations on tensors can be performed on a GPU using NVIDIA's CUDA computing platforms (https://en.wikipedia.org/wiki/CUDA).\n",
    "\n",
    "The different processors that can be used for calculations, i.e. CPU or GPU, are represented in `pytorch` as devices. Each tensor has an associated device on which its data is located.\n",
    "The default device is represented by `torch.device(\"cpu\")`. Hence by default, all calculations are executed on the CPU.  In order to be able to perform calculations on a tensor using a GPU, you need to move its data to the GPU's memory.\n",
    "\n",
    "\n",
    "> **Note:** If you are using Google Colab, you may need to enable GPU hardware acceleration by `Go to Menu > Runtime > Change runtime` and change hardware acceleration to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjlISwlo8eAS",
    "outputId": "4235de53-4286-45e1-de4c-e7b681f97c1a"
   },
   "outputs": [],
   "source": [
    "# First check if CUDA is available.\n",
    "print(torch.cuda.is_available())\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-NH9LsS8eAU"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "To train a network on the `catsndogs` dataset, we need to load the images into tensors. The `catsndogs.training` module has an attribute `folder`, which points to the root folder containing the training data. The root folder contains a `cat` and a `dog` folder which holds the images of cats and dogs, respectively.\n",
    "\n",
    "\n",
    "Using the `torchvision.datasets.ImageFolder` class, data that is organized in a folder structure like this can be turned directly into a dataset for training ML algorithms. The dataset provides access to the images as input and as an integer representations of the class labels as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz4pO5_98eAV"
   },
   "outputs": [],
   "source": [
    "from catsndogs.training import folder\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "images = ImageFolder(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2K3HSXm8eAV"
   },
   "source": [
    "You can load a sample from the training data by indexing the `images` object, which will return a tuple `(image, label)` containing the loaded image and corresponding label, which is 0 for cat and 1 for dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "4zRqhcxq8eAV",
    "outputId": "3e933953-3749-4564-b313-7b56dd880ab3"
   },
   "outputs": [],
   "source": [
    "image, label = images[0]\n",
    "plt.imshow(image)\n",
    "print(\"The type of image is:\", type(image))\n",
    "print(\"The label is:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R-Y7HAg8eAW"
   },
   "source": [
    "However, as the code above shows, the type of the loaded image is a `PIL` image and not a tensor. To automatically transform the loaded image into a tensor, you can make use of the `transformation` parameter of the `ImageFolder` class.\n",
    "\n",
    "The cell below adds a composition of two transforms to the dataset. The two transforms are applied sequentially to the image object that would otherwise be returned from the dataset. The first transform turns the image into a torch tensor and the second transform normalizes the image values so that they lie in the range $[-1, 1]$.\n",
    "\n",
    "> Note: Input data that is not centered around zero can cause convergence problems during training, so it is usually a good idea to normalize input data to a range centered around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URysZcXe8eAW",
    "outputId": "c13577df-f21b-4b1e-edae-1e18ae28af0d"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "transform = Compose([ToTensor(),\n",
    "                     Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "images = ImageFolder(folder, transform=transform)\n",
    "image, label = images[0]\n",
    "print(\"Type of image is now:\", type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pk-zNLQ48eAW"
   },
   "outputs": [],
   "source": [
    "# This function inverts the transformation of the input images.\n",
    "def to_image(tensor):\n",
    "    tensor = 0.5 * (tensor + 1.0)\n",
    "    return to_pil_image(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cP-H0EI8eAX"
   },
   "source": [
    "For the training, we further split the data into training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O38SrTom8eAX"
   },
   "outputs": [],
   "source": [
    "n_train = int(0.9 * len(images))\n",
    "n_val = len(images) - n_train\n",
    "training_data_catsndogs, validation_data_catsndogs = torch.utils.data.random_split(images, (n_train, n_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkoIRg2r8eAX"
   },
   "source": [
    "## Exercise 6: Training a fully-connected network [4 points]\n",
    "### Defining a neural network model\n",
    "\n",
    "Neural networks in `pytorch` are represented using the [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class. The typical way to define a neural network model is to define a new class that inherits from the `Module` class.\n",
    "\n",
    "### a) [1 point]\n",
    "\n",
    "Inspect the code given below and, using the documentation of the [`torch.nn`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) module, answer the following questions:\n",
    "- What is the architecture of instances of the `FullyConnected` class?\n",
    "- What activations functions are applied in the hidden layers?\n",
    "- What activation function is used for the output?\n",
    "- How are the parameters of the network initialized? Why is this important to know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW0oNeWO8eAY"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    \"\"\"\n",
    "    Usually, this docstring should contain useful information about this\n",
    "    class but this would make the exercise too easy.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_features,\n",
    "                 width):\n",
    "        \"\"\"\n",
    "        Create a new mysterious network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_features = input_features\n",
    "        self.fc_1 = nn.Linear(input_features, width)\n",
    "        self.fc_2 = nn.Linear(width, width)\n",
    "        self.fc_3 = nn.Linear(width, width)\n",
    "        self.fc_4 = nn.Linear(width, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward method required by nn.Module base class.\n",
    "        \"\"\"\n",
    "        x = x.flatten(1, -1)\n",
    "        x = self.fc_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc_3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc_4(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI1xt4108eAY"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UG5Z0IY8eAY"
   },
   "source": [
    "### The training loop\n",
    "\n",
    "In the cell below you find code for a typical training loop in `pytorch`. \n",
    "\n",
    "### b) [1 point]\n",
    "\n",
    "Look at the function below and answer the following questions: \n",
    "\n",
    "- Most of the actual training functionality is abstracted away in the arguments provided to the function. For each of the arguments, describe what tasks the corresponding object has to perform so that this method can be used to train a neural network.\n",
    "- What functions do the calls `model.train()` and `model.eval()` have? Why are these calls important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLer04sL8eAY"
   },
   "outputs": [],
   "source": [
    "def train_epoch(training_loader,\n",
    "                validation_loader,\n",
    "                model,\n",
    "                loss,\n",
    "                optimizer,\n",
    "                device):\n",
    "    \"\"\"\n",
    "    Again, this should be a useful docstring, but that would\n",
    "    give away the answer for the exercise.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    training_loss = 0.0\n",
    "    n = len(training_loader)\n",
    "    \n",
    "    for i, (x, y) in enumerate(training_loader):\n",
    "        \n",
    "        # Set gradients to zero.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move input to device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Predict output, compute loss, perform optimizer step.\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y.view(-1, 1).float())\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += l.item()\n",
    "        print(\"Batch ({} / {}): Loss {:.2f}\".format(i, n, l.item()), end=\"\\r\")\n",
    "        \n",
    "    training_loss /= n\n",
    "        \n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    n = len(validation_loader)\n",
    "    \n",
    "    for i, (x, y) in enumerate(validation_loader):\n",
    "        # Move input to device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Predict output, compute loss, perform optimizer step.\n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y.view(-1, 1).float())\n",
    "        \n",
    "        validation_loss += l.item()\n",
    "    validation_loss /= n\n",
    "    \n",
    "    model.to(torch.device(\"cpu\"))\n",
    "    \n",
    "    return (training_loss, validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgS0Opex8eAZ"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoOdjwi28eAZ"
   },
   "source": [
    "## The optimizer object\n",
    "\n",
    "In the code above the optimization method was hidden in the `optimizer` object. To understand how to write a suitable optimizer, you first need to understand a bit more about the role of `Module` objects in `pytorch`. The `torch.nn.Module` class is the base class for all neural networks and the components that make up neural networks. Module objects typically have trainable parameters. These trainable parameters of a module can be accessed via its `parameters()` member function. When a module contains attributes that are themselves `Module` instances, then  the `parameters()` function of the containing module will automatically list the trainable parameters of its `Module` attributes.\n",
    "\n",
    "In order to  train a network, the optimizer needs to be aware of the module's parameters. In `pytorch` an optimizer object therefore always needs to be instantiated with a list of parameters that should be trained. In addition to that, an optimizer typically provides a function to set the gradients of the module parameters to zero. This is because gradients in pytorch are accumulated between consecutive calls to the `backward()` function.\n",
    "This makes it necessary to set the gradients to zero between two training iterations.\n",
    "\n",
    "Note here how the the `step` method of the `SGD` class performs gradient descent on the provided list of parameters.\n",
    "\n",
    "> The `parameters()` member function returns a list of tensors representing the weight matrices and bias vectors in a network. Given a tensor `p`, you can access its gradients using the `p.grad` attribute.\n",
    "\n",
    "> Because of the way `pytorch`'s autograd function works, changing the  value of a parameter `p` has to be done using its `p.data` attribute:\n",
    "\n",
    "```\n",
    "p.data = ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iy1bpiFW8eAa"
   },
   "outputs": [],
   "source": [
    "class GradientDescent():\n",
    "    \"\"\"\n",
    "    A gradient descent optimizer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 parameters,\n",
    "                 learning_rate):\n",
    "        \"\"\"\n",
    "        Create a gradient descent optimizer.\n",
    "        \n",
    "        Arguments:\n",
    "            parameters: Iterable providing the parameters to optimize.\n",
    "            learning_rate: The learning rate to use for optimization.\n",
    "        \"\"\"\n",
    "        self.parameters = list(parameters)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters:\n",
    "            if not p.grad is None:\n",
    "                p.grad.zero_()\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a gradient descent step on parameters associated to this optimizer.\n",
    "        \"\"\"\n",
    "        for p in self.parameters:\n",
    "            p.data.add_(p.grad, alpha=-self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3nwYsLR8eAa"
   },
   "source": [
    "## Training the network\n",
    "\n",
    "With the optimizer, the model and the training loop in place we are close to being able to start training the network, however a few details remain to be sorted out.\n",
    "\n",
    "The `training_data_catsndogs` and `validation_data_catsndogs` object defined above can be used to iterate over the data, but only on a per sample basis. For the training of a neural network, however, we typically want to iterate through the data in batches. To take care of this, `pytorch` provides the `DataLoader` class, which can be used to batch and shuffle existing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICkUvYS88eAa"
   },
   "outputs": [],
   "source": [
    "from  torch.utils.data import DataLoader\n",
    "training_loader_catsndogs = DataLoader(training_data_catsndogs, batch_size=32, shuffle=True)\n",
    "validation_loader_catsndogs = DataLoader(validation_data_catsndogs, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdSImGwx8eAa"
   },
   "source": [
    "Next, we need to choose a suitable training loss to minimize. We will use the Binary Cross Entropy with logits loss that combines a Sigmoid layer and the BCELoss in one single class. \n",
    "\n",
    "You can find and read more about other loss functions under [`torch.nn`](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "> Note that the loss function is applied **directly** to the output of the network in the `train_epoch` function defined above. That is why we use a loss that also includes a sigmoid function. If we take the output as-is, it has the potential to be any real-valued number. By applying the sigmoid function, we bound the output to values between [0,1], which better corresponds to a predicted probability of cat (0) vs. dog (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5qudxxV8eAb"
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZCtXPqo8eAb"
   },
   "source": [
    "Finally, we choose the device to run the training on. If available, you should use a GPU because it will be substantially faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcjWEMSh8eAb"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # Default CUDA device\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8zLAavy8eAb"
   },
   "source": [
    "### c) [1 point]\n",
    "\n",
    "Train the neural network for at least 10 epochs, then reduce the learning rate and continue training for at least another 10 epochs. Plot the resulting training and validation losses, and answer the following question(s):\n",
    "\n",
    "- Was the training successful? Explain why.  \n",
    "- If the training was unsuccessful, give suggestion(s) on what can be done to make the training successful over the same number of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABauijh0nQkL"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l892iZ1BnQkL"
   },
   "outputs": [],
   "source": [
    "fc_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh6CT0Fp8eAc"
   },
   "source": [
    "### d) [0.5 point]\n",
    "\n",
    "A useful performance metric for binary classification tasks  is the [receiver operating characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). Complete the code below and write a function that computes the true positive and false positive rate for varying values of the discrimination threshold $p \\in [0, 1]$. Then, using the code below, plot the ROC curve. What is the significance of the black, dashed line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sF0PqNBnQkM"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nBUqo1UnQkM"
   },
   "outputs": [],
   "source": [
    "def receiver_operating_characteristic(model,\n",
    "                                      validation_loader,\n",
    "                                      ps):\n",
    "    \"\"\"\n",
    "    Computes receiver operating characteristic for given model and\n",
    "    validation data.\n",
    "    \n",
    "    Arguments:\n",
    "        model: The pytorch model to evaluate.\n",
    "        validation_loader: torch DataLoader to use to iterate over validation data.\n",
    "        ps: Iterable containing the values of the discrimination threshold in\n",
    "           increasing order.\n",
    "    Returns:\n",
    "        (fpr, tpr): Tuple containing the false positive rates (fpr) and the true\n",
    "            positive rates as numpy.ndarrays.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr = receiver_operating_characteristic(fc_model, validation_loader_catsndogs, np.linspace(0, 1, 101))\n",
    "\n",
    "def plot_ROC(fpr,tpr):\n",
    "    \"\"\"\n",
    "    Plots ROC curve\n",
    "\n",
    "    Arguments:\n",
    "        fpr: array-like containing false positive rates\n",
    "        tpr: array-like containing true positive rates\n",
    "\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 1, 101)\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(x, x, c=\"k\", ls=\"--\")\n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.set_ylabel(\"TPR\")\n",
    "    ax.set_xlabel(\"FPR\")\n",
    "    ax.set_title(\"Receiver operator characteristic\")\n",
    "\n",
    "plot_ROC(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p69zV4Za8eAd"
   },
   "source": [
    "### e) [0.5 point]\n",
    "\n",
    "One way to summarize the receiver operator characteristic (ROC) is to compute the area under the curve. This can be done using the [trapezoidal rule](https://en.wikipedia.org/wiki/Trapezoidal_rule). Complete the code below and write a function for computing the area under the receiver operator characteristic curve (AUC ROC) using the trapezoidal rule. Then, calculate the area under the ROC-curve in exercise above (e) using this function.\n",
    "\n",
    "Note: You are not allowed to use any built-in trapeziodal function (e.g. the one from NumPy). You are to make your own implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGviGeNRnQkM"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pbdOBWinQkN"
   },
   "outputs": [],
   "source": [
    "def auc_roc(fpr, tpr):\n",
    "    \"\"\"\n",
    "    Computes area under receiver operating characteristic curve using the\n",
    "    trapeziodal rule for given false positive and true positive rates.\n",
    "    \n",
    "    Arguments:\n",
    "        fpr: False positive rates.\n",
    "        tpr: True positive rates.\n",
    "        \n",
    "    Returns:\n",
    "        auc_roc: Area under the receiver operating characteristic curve as\n",
    "            float value\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sckQV_lg8eAd"
   },
   "source": [
    "Finally, let's look at the prediction for samples from the validation set. Is this what you expected? You do **not** need to write down an answer for this question, but you should always spend some time to reflect on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwtbdOJ78eAd"
   },
   "outputs": [],
   "source": [
    "def index_to_pet(index):\n",
    "    \n",
    "    if index == 0:\n",
    "        return \"cat\"\n",
    "    else:\n",
    "        return \"dog\"\n",
    "    \n",
    "def plot_results(model, validation_data):\n",
    "    model.to(torch.device(\"cpu\"))\n",
    "    f, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    for i in range(10):\n",
    "\n",
    "        # Make prediction on random validation sample\n",
    "        index = np.random.randint(len(validation_data))\n",
    "        x, y = validation_data[index]\n",
    "        c = torch.sigmoid(model(x.unsqueeze(0))) >= 0.5\n",
    "        x = 0.5 * (x + 1.0)\n",
    "        \n",
    "        ax = axs.ravel()[i]\n",
    "        ax.imshow(to_pil_image(x))\n",
    "        title = \"Predicted '{}', \\n True '{}'\"\n",
    "        title = title.format(index_to_pet(c), index_to_pet(y))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "        \n",
    "plot_results(fc_model, validation_data_catsndogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr75_WS78eAd"
   },
   "source": [
    "## Exercise 7: Training a convolutional neural network [3 points]\n",
    "\n",
    "### a) [1 point]\n",
    "\n",
    "Define and train a convolutional network with the following architecture:\n",
    "- 2D conv. layer: $8$ filters, kernel size $5 \\times 5$, stride 1\n",
    "- ReLU activation function\n",
    "- Max pooling: kernel size $4 \\times 4$, stride 4\n",
    "- 2D conv. layer: $4$ filters, kernel size $5 \\times 5$, stride 1\n",
    "- ReLU activation function\n",
    "- Max pooling: kernel size $3 \\times 3$, stride 3\n",
    "- Fully connected layer: 1 neuron\n",
    "    \n",
    "\n",
    "> **Hint:** You can find all necessary components to implement the convolutional network in the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv0QIM66nQkN"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMAgt75inQkN"
   },
   "outputs": [],
   "source": [
    "conv_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aN-ax-DY8eAe"
   },
   "source": [
    "### b)  [2 points]\n",
    "\n",
    "Tune the network architecture and training routine to achieve a validation loss lower than 0.3. Try to do at least 2 improvements. Some things you may want to try: \n",
    "- Alter the complexity of your network (number of layers, filters or neurons)\n",
    "- A learning rate schedule\n",
    "- Checkpoints or early stopping\n",
    "- Data augmentation to increase the number of training points\n",
    "- Dropout\n",
    "- Other regularization techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIuLi3NdnQkO"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TzsuTOrnQkO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeqjK9ZU8eAf"
   },
   "source": [
    "## Exercise 8: Evaluation on test set [1 point]\n",
    "\n",
    "Now evaluate the performance of the fully-connected neural network to your best convolutional neural network on the `catsndogs` test data, which is available in `catsndogs.test` module.\n",
    "\n",
    "- Plot ROC curves and compute AUC ROC for both the fully-connected and the convolutional model.\n",
    "- Compute the accuracy of each model for a discimination threshold p = 0.5\n",
    "- Provide a plot, for each model, of 8 images from the test set together with the prediction from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OADbtGIvnQkO"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IU6dP7RmnQkO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPyesZLhnQkP"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBFB8bvJnQkP"
   },
   "source": [
    "## Clustering\n",
    "\n",
    "We will also consider different clustering approaches to the catsndogs dataset. Two very performant deep learning models for image classification are [CLIP](https://openai.com/blog/clip/) and [Faster R-CNN](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn). One potential explanation for the success of these networks is that they are good at forming internal _latent representations_ of the input images they receive which allows them to perform successful classification. We will take a closer look at the representations of these models and perform some clustering on the representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ucxi3aknQkP"
   },
   "source": [
    "### Load representations\n",
    "\n",
    "The generated representations and predicitons of these models for the training split of the `catsndogs` data are readibly available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7pXFm5snQkP"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from catsndogs.training import folder\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = ImageFolder(folder)\n",
    "clip_preds = np.load(\"clip_preds.npy\")\n",
    "clip_reps = np.load(\"clip_representations.npy\")\n",
    "frcnn_preds = np.load(\"frcnn_preds.npy\")\n",
    "frcnn_reps = np.load(\"frcnn_reps.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GkGqWdFnQkP"
   },
   "source": [
    "### Model performance \n",
    "We can start by confirming that these models perform very well on the catsndogs classification task despite not having been fine-tuned. CLIP makes an incorrect prediction on only four out of 2947 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gANXYywHnQkP",
    "outputId": "a20f8063-9372-464c-ea5b-06fb09a30255"
   },
   "outputs": [],
   "source": [
    "true_y = np.array([val[1] for val in images])\n",
    "clip_score = np.sum(clip_preds==true_y)\n",
    "print(f\"CLIP # of correct predictions: {clip_score} out of {len(true_y)}\")\n",
    "frcnn_score = np.sum(frcnn_preds==true_y)\n",
    "print(f\"FRCNN # of correct predictions: {frcnn_score} out of {len(true_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-7Dq6pBnQkP"
   },
   "source": [
    "### Dimensionality reduction\n",
    "\n",
    "The size of the image representations for CLIP and Faster R-Cnn is 512 and 64 respectively. To allow for easier visual inspection, we will perform clustering in only two dimensions and thus need to employ dimensionality reduction. We will make use of Principal Component Analysis (PCA) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jppS93ignQkP",
    "outputId": "6010249a-d4c1-4ab5-d0a1-30b9fba4a2bd"
   },
   "outputs": [],
   "source": [
    "clip_pca = PCA(n_components=2, svd_solver='full')\n",
    "clip_pca_feats = clip_pca.fit_transform(clip_reps)\n",
    "\n",
    "frcnn_pca = PCA(n_components=2, svd_solver='full')\n",
    "frcnn_pca_feats = frcnn_pca.fit_transform(frcnn_reps)\n",
    "\n",
    "cluster_data = pd.DataFrame({\"true_y\": true_y, \n",
    "                             \"clip_preds\": clip_preds, \n",
    "                             \"frcnn_preds\": frcnn_preds.astype(np.int32),\n",
    "                             \"clip_pca_1\": clip_pca_feats[:,0],\n",
    "                             \"clip_pca_2\": clip_pca_feats[:,1],\n",
    "                             \"frcnn_pca_1\": frcnn_pca_feats[:,0],\n",
    "                             \"frcnn_pca_2\": frcnn_pca_feats[:,1]}\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=cluster_data, x=\"clip_pca_1\", y=\"clip_pca_2\"\n",
    ")\n",
    "plt.title(\"CLIP features PCA components\")\n",
    "plt.xlim([-0.4,0.45])\n",
    "plt.ylim([-0.4,0.45])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=cluster_data, x=\"frcnn_pca_1\", y=\"frcnn_pca_2\"\n",
    ")\n",
    "plt.title(\"FRCNN features PCA components\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v24O7rgCnQkQ"
   },
   "source": [
    "### Density plots\n",
    "Many data points overlap to form densities. Thus, it might be suitable to consider the density plots of the data to get a better grasp of its underlying distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWcVohhbnQkQ",
    "outputId": "c5001e66-bf15-41ba-ae4a-36f98b3f5501"
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=cluster_data, x=\"clip_pca_1\", y=\"clip_pca_2\",\n",
    "    levels=20, thresh=.2,\n",
    ")\n",
    "plt.axis\n",
    "plt.title(\"CLIP features PCA components\")\n",
    "plt.xlim([-0.4,0.45])\n",
    "plt.ylim([-0.4,0.45])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(\n",
    "    data=cluster_data, x=\"frcnn_pca_1\", y=\"frcnn_pca_2\",\n",
    "    levels=20, thresh=.2,\n",
    ")\n",
    "plt.title(\"FRCNN features PCA components\")\n",
    "plt.xlim([-10,10])\n",
    "plt.ylim([-10,10])\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfMJfXNWnQkQ"
   },
   "source": [
    "## Exercise 9: Choose clustering approach [1 point]\n",
    "\n",
    "We will now start to perform the clustering. Based on the plots for the CLIP and FRCNN representations above, what clustering approach out of K-Means or EM with GMM would you use to try to cluster each of this data? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxm4Wy89nQkQ"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11Kq02JunQkQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnN3rABJnQkQ"
   },
   "source": [
    "## Exercise 10: Implement K means [2 points]\n",
    "\n",
    "Regardless of your answer above, we will start by implementing K-means. \n",
    "\n",
    "* Implement the basic (linear) $k$-means algorithm as described in the lecture, using the euclidean distance. \n",
    "* Use random points sampled uniformly without replacement from the data as initialization for the centroids. \n",
    "* Terminate the iterative procedure when the cluster assignments do not change. \n",
    "* Note that you do not need to make use of the data yet at this step (unless you want to do some testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2f3GyfOnQkQ"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7Ruo48xnQkR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhSaak-fnQkR"
   },
   "source": [
    "## Exercise 11: Run your K means implementation [1 point]\n",
    "\n",
    "In this exercise you will run your $k$-means algorithm implementation on the CLIP and FRCNN features respectively. You should run the algorithm on the `pca_1` and `pca_2` features. For each of the CLIP and FRCNN features:\n",
    "\n",
    "1. What value of $k$ do you think is suitable for this data?\n",
    "\n",
    "2. Run your k-means implementation on the data with your choice of suitable $k$. Run the algorithm 3 times on the data and plot the resulting cluster assignments for each run. Also note for each run how many iterations were required before convergence. \n",
    "\n",
    "3. Do you always get the same final cluster solutions and same number of required iterations? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teWtvjGKnQkR"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AuPO2oyVnQkR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_13Y6v1BnQkS"
   },
   "source": [
    "## Exercise 12: Implement the intra-cluster variance metric [1 point]\n",
    "\n",
    "* We will now estimate a good value for the number of clusters $k$ for the CLIP data using the intra-cluster variance $W=\\sum_k W_k$, where $W_k = \\frac{1}{|C_k|} \\sum_{x\\in C_k}(x-\\mu_k)^2$.\n",
    "* Plot the intra-cluster variance of your clustering solutions for $k \\in [1,...,6]$ for the CLIP data.\n",
    "* What $k$ seems to be most suitable, based on the intra-cluster variance plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_t2hyRWnQkS"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TagckpsqnQkS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaOoPJq-nQkS"
   },
   "source": [
    "## Exercise 13: Plot a GMM [0.5 points]\n",
    "\n",
    "We will now consider a mixture model. The probability of an observation $x \\in \\mathbb{R}^D$ is given by: $$p(x) = \\sum_{k=1}^{K} \\pi_k P(x|\\theta_k)$$ where $\\pi_k$ are the probabilities a priori and $P(x|\\theta_k)$ are multi-dimensional Gaussian characterized by their mean $\\mu_k$ and their co-variance matrix $\\Sigma_k$\n",
    ", i.e. $\\theta_k = (\\mu_k, \\Sigma_k)$.\n",
    "\n",
    "Plot the probability distribution $p(x)$ for the following parameter values.\n",
    "\n",
    "$D = 1$,\n",
    "\n",
    "$K = 2$,\n",
    "\n",
    "$\\pi_1 = \\pi_2 = 0.5$,\n",
    "\n",
    "$\\mu_1 = -2$,\n",
    "\n",
    "$\\mu_2 = 5$, \n",
    "\n",
    "$\\Sigma_1 = 3$,\n",
    "\n",
    "$\\Sigma_2 = 5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2amS9a-bnQkS"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66h1fyBnnQkT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzOUvX8ynQkU"
   },
   "source": [
    "## Exercise 14: Fit a GMM with the EM algorithm [1 point]\n",
    "\n",
    "We will not implement our own GMM or EM algorithm for this task, but use an already existing implementation from scikit-learn. Fit a Gaussian Mixture model from scikit-learn to each of the CLIP and FRCNN features and plot the resulting clusters. You can find the documentation for the function [here](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQWy2NffnQkU"
   },
   "source": [
    "#### üìù Your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0tkvDDfnQkU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzygv_CUnQkU"
   },
   "source": [
    "## Final note \n",
    "\n",
    "We can use the true y labels of the catsndogs data to get one \"correct\" clustering of the data points. As can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7k8XbfGMnQkU",
    "outputId": "fd70880e-cf04-40de-f379-5992d728047e"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=cluster_data, x=\"clip_pca_1\", y=\"clip_pca_2\", hue=\"true_y\",\n",
    ")\n",
    "plt.title(\"CLIP features PCA components\")\n",
    "plt.show()\n",
    "sns.scatterplot(\n",
    "    data=cluster_data, x=\"frcnn_pca_1\", y=\"frcnn_pca_2\", hue=\"true_y\"\n",
    ")\n",
    "plt.title(\"FRCNN features PCA components\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6921002a43cd2c0715c7f002ee6db9fc0e92ab1102cc0e6b3eeec79561337635"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
